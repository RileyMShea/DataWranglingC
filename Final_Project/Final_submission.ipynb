{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# OpenStreetMap Data Case Study\n\n### Map Area\nRochester, NY United States\n\n- [https://www.openstreetmap.org/export#map\u003d12/43.1575/-77.6153](https://www.openstreetmap.org/relation/177415)\n\n\nRochester, NY is my hometown so I thought it would interesting to see if I could learn more about the surrounding area\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Problems Encountered in the Map\nAfter downloading the osm file of the Rochester area and running it against a provisional data.py file, I noticed five main problems with the data, which I will discuss in the following order:\n\n- Inconsistent postal codes *(“1445033”, “14607-2082”, “14617-1822”)*\n- Over­abbreviated street names *(“S Tryon St Ste 105”)*\n- “Incorrect” postal codes (Charlotte area zip codes all begin with “282” however a large portion of all documented zip codes were outside this region.)\n- Second­ level `“k”` tags with the value `\"type\"`(which overwrites the element’s previously processed `node[“type”]field`).\n- Street names in second ­level `“k”` tags pulled from Tiger GPS data and divided into segments, in the following format:\n\n    ```XML\n\t\u003ctag k\u003d\"tiger:name_base\" v\u003d\"Stonewall\"/\u003e \n\t\u003ctag k\u003d\"tiger:name_direction_prefix\" v\u003d\"W\"/\u003e \n\t\u003ctag k\u003d\"tiger:name_type\" v\u003d\"St\"/\u003e\n\t```\n### Over­abbreviated Street Names\nOnce the data was imported to SQL, some basic querying revealed street name abbreviations and postal code inconsistencies. To deal with correcting street names, I opted not use regular expressions, and instead iterated over each word in an address, correcting them to their respective mappings in audit.py using the following function:\n\n```python \ndef update(name, mapping): \n\twords \u003d name.split()\n\tfor w in range(len(words)):\n\t\tif words[w] in mapping:\n\t\t\tif words[w­1].lower() not in [\u0027suite\u0027, \u0027ste.\u0027, \u0027ste\u0027]: \n\t\t\t\t# For example, don\u0027t update \u0027Suite E\u0027 to \u0027Suite East\u0027\n\t\t\t\twords[w] \u003d mapping[words[w]] name \u003d \" \".join(words)\n\treturn name\n```\n\nThis updated all substrings in problematic address strings, such that:\n*“S Tryon St Ste 105”*\nbecomes:\n*“South Tryon Street Suite 105”*\n\n### Postal Codes\nPostal code strings posed a different sort of problem, forcing a decision to strip all leading and trailing characters before and after the main 5­digit zip code. This effectively dropped all leading state characters (as in “NC28226”) and 4­digit zip code extensions following a hyphen (“28226­0783”). This 5­digit restriction allows for more consistent queries.\n\n\nRegardless, after standardizing inconsistent postal codes, some altogether “incorrect” (or perhaps misplaced?) postal codes surfaced when grouped together with this aggregator:\n\n```sql\nSELECT tags.value, COUNT(*) as count \nFROM (SELECT * FROM nodes_tags \n\t  UNION ALL \n      SELECT * FROM ways_tags) tags\nWHERE tags.key\u003d\u0027postcode\u0027\nGROUP BY tags.value\nORDER BY count DESC;\n```\n\nHere are the top ten results, beginning with the highest count:\n\n```sql\nvalue|count\n28205|900\n28208|388\n28206|268\n28202|204\n28204|196\n28216|174\n28211|148\n28203|120\n28209|104\n28207|86\n```\n\n These results were taken before accounting for Tiger GPS zip codes residing in second­ level “k” tags. Considering the relatively few documents that included postal codes, of those, it appears that out of the top ten, seven aren’t even in Charlotte, as marked by a “#”. That struck me as surprisingly high to be a blatant error, and found that the number one postal code and all others starting with“297”lie in Rock Hill, SC. So, I performed another aggregation to verify a certain suspicion...\n# Sort cities by count, descending\n\n```sql\nsqlite\u003e SELECT tags.value, COUNT(*) as count \nFROM (SELECT * FROM nodes_tags UNION ALL \n      SELECT * FROM ways_tags) tags\nWHERE tags.key LIKE \u0027%city\u0027\nGROUP BY tags.value\nORDER BY count DESC;\n```\n\nAnd, the results, edited for readability:\n\n```sql\nRock Hill   111       \nPineville   27        \nCharlotte   26        \nYork        24        \nMatthews    10        \nConcord     4         \n3000        3         \n10          2         \nLake Wylie  2         \n1           1         \n3           1         \n43          1         \n61          1         \nBelmont, N  1         \nFort Mill,  1         \n```\n\nThese results confirmed my suspicion that this metro extract would perhaps be more aptly named “Metrolina” or the “Charlotte Metropolitan Area” for its inclusion of surrounding cities in the sprawl. More importantly, three documents need to have their trailing state abbreviations stripped. So, these postal codes aren’t “incorrect,” but simply unexpected. However, one final case proved otherwise.\nA single zip code stood out as clearly erroneous. Somehow, a “48009” got into the dataset. Let’s display part of its document for closer inspection (for our purposes, only the “address” and “pos” fields are relevant):\n\n```sql\nsqlite\u003e SELECT *\nFROM nodes \nWHERE id IN (SELECT DISTINCT(id) FROM nodes_tags WHERE key\u003d\u0027postcode\u0027 AND value\u003d\u002748009\u0027)\n```\n`1234706337|35.2134608|-80.8270161|movercash|433196|1|7784874|2011-04-06T13:16:06Z`\n\n`sqlite\u003e SELECT * FROM nodes_tags WHERE id\u003d1234706337 and type\u003d\u0027addr\u0027;`\n\n```sql\n1234706337|housenumber|280|addr\n1234706337|postcode|48009|addr\n1234706337|street|North Old Woodward Avenue|addr\n```\n\n It turns out, *“280 North Old Woodward Avenue, 48009”* is in Birmingham, Michigan. All data in this document, including those not shown here, are internally consistent and verifiable, except for the latitude and longitude. These coordinates are indeed in Charlotte, NC. I’m not sure about the source of the error, but we can guess it was most likely sitting in front of a computer before this data entered the map. The document can be removed from the database easily enough.\n\n# Data Overview and Additional Ideas\nThis section contains basic statistics about the dataset, the MongoDB queries used to gather them, and some additional ideas about the data in context.\n\n### File sizes\n```\ncharlotte.osm ......... 294 MB\ncharlotte.db .......... 129 MB\nnodes.csv ............. 144 MB\nnodes_tags.csv ........ 0.64 MB\nways.csv .............. 4.7 MB\nways_tags.csv ......... 20 MB\nways_nodes.cv ......... 35 MB  \n```  \n\n### Number of nodes\n```\nsqlite\u003e SELECT COUNT(*) FROM nodes;\n```\n1471350\n\n### Number of ways\n```\nsqlite\u003e SELECT COUNT(*) FROM ways;\n```\n84502\n\n### Number of unique users\n```sql\nsqlite\u003e SELECT COUNT(DISTINCT(e.uid))          \nFROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) e;\n```\n337\n\n### Top 10 contributing users\n```sql\nsqlite\u003e SELECT e.user, COUNT(*) as num\nFROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e\nGROUP BY e.user\nORDER BY num DESC\nLIMIT 10;\n```\n\n```sql\njumbanho    823324    \nwoodpeck_f  481549    \nTIGERcnl    44981     \nbot-mode    32033     \nrickmastfa  18875     \nLightning   16924     \ngrossing    15424     \ngopanthers  14988     \nKristenK    11023     \nLambertus   8066 \n```\n \n### Number of users appearing only once (having 1 post)\n```sql\nsqlite\u003e SELECT COUNT(*) \nFROM\n    (SELECT e.user, COUNT(*) as num\n     FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e\n     GROUP BY e.user\n     HAVING num\u003d1)  u;\n```\n56\n\n# Additional Ideas\n\n## Contributor statistics and gamification suggestion \nThe contributions of users seems incredibly skewed, possibly due to automated versus manual map editing (the word “bot” appears in some usernames). Here are some user percentage statistics:\n\n- Top user contribution percentage (“jumbanho”) 52.92%\n- Combined top 2 users\u0027 contribution (“jumbanho” and “woodpeck_fixbot”) 83.87%\n- Combined Top 10 users contribution\n94.3%\n- Combined number of users making up only 1% of posts 287 (about 85% of all users)\n\nThinking about these user percentages, I’m reminded of “gamification” as a motivating force for contribution. In the context of the OpenStreetMap, if user data were more prominently displayed, perhaps others would take an initiative in submitting more edits to the map. And, if everyone sees that only a handful of power users are creating more than 90% a of given map, that might spur the creation of more efficient bots, especially if certain gamification elements were present, such as rewards, badges, or a leaderboard. \n\n## Additional Data Exploration\n\n### Top 10 appearing amenities\n\n```sql\nsqlite\u003e SELECT value, COUNT(*) as num\nFROM nodes_tags\nWHERE key\u003d\u0027amenity\u0027\nGROUP BY value\nORDER BY num DESC\nLIMIT 10;\n```\n\n```sql\nplace_of_worship  580       \nschool            402       \nrestaurant        80        \ngrave_yard        75        \nparking           63        \nfast_food         51        \nfire_station      48        \nfuel              31        \nbench             30        \nlibrary           28 \n```\n\n### Biggest religion (no surprise here)\n\n```sql\nsqlite\u003e SELECT nodes_tags.value, COUNT(*) as num\nFROM nodes_tags \n    JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value\u003d\u0027place_of_worship\u0027) i\n    ON nodes_tags.id\u003di.id\nWHERE nodes_tags.key\u003d\u0027religion\u0027\nGROUP BY nodes_tags.value\nORDER BY num DESC\nLIMIT 1;\n```\n`christian   571`\n\n### Most popular cuisines\n\n```sql\nsqlite\u003e SELECT nodes_tags.value, COUNT(*) as num\nFROM nodes_tags \n    JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value\u003d\u0027restaurant\u0027) i\n    ON nodes_tags.id\u003di.id\nWHERE nodes_tags.key\u003d\u0027cuisine\u0027\nGROUP BY nodes_tags.value\nORDER BY num DESC;\n```\n\n```sql\namerican    9         \npizza       5         \nsteak_hous  4         \nchinese     3         \njapanese    3         \nmexican     3         \nthai        3         \nitalian     2         \nsandwich    2         \nbarbecue    1\n```\n\n# Conclusion\n After this review of the data it’s obvious that the Charlotte area is incomplete, though I believe it has been well cleaned for the purposes of this exercise. It interests me to notice a fair amount of GPS data makes it into OpenStreetMap.org on account of users’ efforts, whether by scripting a map editing bot or otherwise. With a rough GPS data processor in place and working together with a more robust data processor similar to data.pyI think it would be possible to input a great amount of cleaned data to OpenStreetMap.org.\t",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}